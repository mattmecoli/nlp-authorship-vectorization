{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path  \n",
    "from gensim import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MarkTwain', 'JohnLocke', 'NathanielHawthorne', 'FScottFitzgerald', 'VirginiaWoolf', 'MaryWollstonecraft', 'JaneAustin', 'EdithWharton', 'MaryShelley', 'KateChopin', 'MargaretFuller', 'HenryDavidThoreau', 'JackLondon', 'CharlesDickens'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning and Tokenizing Words for train texts\n",
    "\n",
    "train_folder = Path.cwd().joinpath(\"rawtextfiles\")\n",
    "\n",
    "clean_books = {}\n",
    "\n",
    "for filename in train_folder.iterdir():\n",
    "    with open(str(filename), encoding='utf-8', errors='ignore') as fhand:\n",
    "        text = fhand.readlines()\n",
    "        clean_text = []\n",
    "        for line in text:\n",
    "            clean_line = re.sub('[^A-Za-z0-9 ]+', '', line)\n",
    "            words = clean_line.split(\" \")\n",
    "            for word in words[:]:\n",
    "                if len(word) != 0:\n",
    "                    clean_text.append(word)\n",
    "        if len(clean_text) > 100 :\n",
    "            file_name = str(filename).split('-')[1]\n",
    "            clean_books[file_name] = clean_books.get(file_name, []) + clean_text[350:]\n",
    "        #         clean_books[file_name] = clean_text[350:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "for k, v in clean_books.items():\n",
    "    if len(v) < 1:\n",
    "        del clean_books[k]     \n",
    "\n",
    "clean_books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186804\n"
     ]
    }
   ],
   "source": [
    "# Checking to make sure additions were correct\n",
    "\n",
    "# for filename in train_folder.iterdir():\n",
    "#      with open(str(filename), encoding='utf-8', errors='ignore') as fhand:\n",
    "#             text = fhand.readlines()\n",
    "#             word_count = []\n",
    "#             for line in text:\n",
    "#                 clean_line = re.sub('[^A-Za-z0-9 ]+', '', line)\n",
    "#                 words = clean_line.split()\n",
    "#                 for word in words:\n",
    "#                     if len(word) != 0:\n",
    "#                         word_count.append(word)\n",
    "        \n",
    "#             print(len(word_count), str(filename).split('/')[7])\n",
    "\n",
    "\n",
    "# len(clean_text)\n",
    "# clean_text[104000:]\n",
    "# file_name\n",
    "print(len(clean_books['MarkTwain']))\n",
    "# clean_books['MarkTwain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition & Classification for Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes about this section:<br>\n",
    "\n",
    "These steps can be time-consuming, but we deemed it an important step in preventing data leakage. While not perfect, the removal of many of the named entities (proper nouns) avoids text-specific named entites like \"Frankenstein\" or period-specific/work-specific named entities like 'King George' from unduly influencing the model (instead of 'style', which is admittedly a hard idea to get at). <br>\n",
    "\n",
    "The corpus undoubtedly missed some named entities, and deleted others. Given more time, we would likely implement an ensemble method of: <br>\n",
    "\n",
    "1) StanfordNER, <br>\n",
    "2) NLTK's built-in POS tagging (based on the Penn Treebank Project model), and <br>\n",
    "3) Polyglot's POS tagger. <br>\n",
    "\n",
    "A word would only be deleted if it appeared in two of the three \"name entity\" lists generated by the methods. <br>\n",
    "\n",
    "<b> Cautionary note </b> that the way this is written is in a preprocessing pipeline mode; i.e. the code as currently written overwrites the original dictionary value, so you would have to rerun prior preprocessing steps each time if you wanted to test different methods. We may change this in future updates. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StanfordNER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger #this is a built in nltk wrapper for StanfordNER which is written in java\n",
    "\n",
    "classifier_doc_path = Path.cwd().joinpath(\"stanford_ner/classifiers/english.muc.7class.distsim.crf.ser.gz\")\n",
    "classifier_dir_path = Path.cwd().joinpath(\"stanford_ner/stanford_ner.jar\")\n",
    "\n",
    "st = StanfordNERTagger(str(classifier_doc_path), str(classifier_dir_path), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CharlesDickens': 8241,\n",
       " 'EdithWharton': 8833,\n",
       " 'FScottFitzgerald': 7113,\n",
       " 'HenryDavidThoreau': 4178,\n",
       " 'JackLondon': 4107,\n",
       " 'JaneAustin': 8495,\n",
       " 'JohnLocke': 422,\n",
       " 'KateChopin': 3833,\n",
       " 'MargaretFuller': 2481,\n",
       " 'MarkTwain': 3040,\n",
       " 'MaryShelley': 3372,\n",
       " 'MaryWollstonecraft': 704,\n",
       " 'NathanielHawthorne': 3690,\n",
       " 'VirginiaWoolf': 10085}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_loss_NER_stan = {}\n",
    "\n",
    "for author in clean_books.keys():\n",
    "    tagged_corpus = st.tag(clean_books[author])\n",
    "    NER_corpus =[word.lower() for word,tag in tagged_corpus if tag == 'O']\n",
    "    clean_books[author] = NER_corpus\n",
    "    word_loss_NER_stan[author] = len(tagged_corpus) - len(NER_corpus)\n",
    "    \n",
    "word_loss_NER_stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CharlesDickens': 2.579189343982674,\n",
       " 'EdithWharton': 3.8442957927309602,\n",
       " 'FScottFitzgerald': 3.4929115453174955,\n",
       " 'HenryDavidThoreau': 2.1618656828401264,\n",
       " 'JackLondon': 2.4174608126482586,\n",
       " 'JaneAustin': 3.5482914318890946,\n",
       " 'JohnLocke': 0.23391553542822618,\n",
       " 'KateChopin': 3.1709657671371136,\n",
       " 'MargaretFuller': 1.4017820316516847,\n",
       " 'MarkTwain': 1.6273741461638938,\n",
       " 'MaryShelley': 1.354706520429071,\n",
       " 'MaryWollstonecraft': 0.5381481283299827,\n",
       " 'NathanielHawthorne': 1.964908543890945,\n",
       " 'VirginiaWoolf': 3.3450307139161235}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_loss_NER_percents_stan = {}\n",
    "\n",
    "for author in word_loss_NER_stan.keys():\n",
    "    word_loss_NER_percents_stan[author] = ((word_loss_NER_stan[author] / (word_loss_NER_stan[author]+len(clean_books[author]))) * 100)\n",
    "\n",
    "word_loss_NER_percents_stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Built-in NLTK POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_loss_NER_nltk = {}\n",
    "\n",
    "for author in clean_books.keys():\n",
    "    tagged_corpus = nltk.tag.pos_tag(clean_books[author])\n",
    "    NER_corpus = [word.lower() for word,tag in tagged_corpus if tag != 'NNP' and tag != 'NNPS']\n",
    "    clean_books[author] = NER_corpus\n",
    "    word_loss_NER_nltk[author] = len(tagged_corpus) - len(NER_corpus)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    \n",
    "word_loss_NER_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_loss_NER_percents_nltk = {}\n",
    "\n",
    "for author in word_loss_NER_nltk.keys():\n",
    "    word_loss_NER_percents_nltk[author] = ((word_loss_NER_nltk[author] / (word_loss_NER_nltk[author]+len(clean_books[author]))) * 100)\n",
    "\n",
    "word_loss_NER_percents_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polyglot POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_loss_NER_poly = {}\n",
    "\n",
    "for author in clean_books.keys():\n",
    "    poly_text_object = Text(' '.join(clean_books[author]))\n",
    "    tagged_corpus = poly_text_object.pos_tags\n",
    "    NER_corpus =[word.lower() for word,tag in tagged_corpus if tag != 'PROPN']\n",
    "    clean_books[author] = NER_corpus\n",
    "    word_loss_NER_poly[author] = len(tagged_corpus) - len(NER_corpus)\n",
    "\n",
    "word_loss_NER_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_loss_NER_poly_percents = {}\n",
    "\n",
    "for author in word_loss_NER.keys():\n",
    "    word_loss_NER_poly_percents[author] = ((word_loss_NER_poly[author] / (word_loss_NER_poly[author]+len(clean_books[author]))) * 100)\n",
    "\n",
    "word_loss_NER_poly_percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics for each method - Note, section not yet complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23391553542822618, 2.2629175711682605, 3.8442957927309602]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_stan_method = word_loss_NER_percents_stan.values()\n",
    "\n",
    "metrics_stan_method_loss = [min(values_stan_method), (sum(values_stan_method)/len(values_stan_method)), max(values_stan_method)]\n",
    "\n",
    "metrics_stan_method_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Method Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing our dataframe with 'paragraphs' from each work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is copied and pasted from the python file \"building_labeled_paragraphs.py\"\n",
    "\n",
    "desired_columns = ['text', 'author', 'sex', 'period']\n",
    "master_paragraphs = pd.DataFrame(columns = desired_columns)\n",
    "\n",
    "def create_paragraphs(corpus, author_name, para_size, num_para):\n",
    "    start_index = random.randint(0, 50)\n",
    "    end_index = start_index + para_size + 1\n",
    "    jump_metric = ((len(corpus)) / (int(num_para)+5))\n",
    "    jump_plus_minus = jump_metric / 10\n",
    "    i = 0\n",
    "    \n",
    "    paragraphs = pd.DataFrame(columns = desired_columns)\n",
    "\n",
    "    for x in range(int(num_para)):\n",
    "        word_slice = corpus[start_index : end_index]\n",
    "        string_paragraph = word_slice[0]\n",
    "        for word in word_slice[1:]:\n",
    "            string_paragraph = string_paragraph + ' ' + word\n",
    "            \n",
    "        paragraphs.loc[i] = [string_paragraph, author_name, None, None]\n",
    "\n",
    "        jump_size = random.randint(int((jump_metric - jump_plus_minus)), int((jump_metric + jump_plus_minus)))\n",
    "        start_index = random.randint(end_index, int(end_index + jump_size))\n",
    "        end_index = start_index + para_size + 1\n",
    "        i += 1\n",
    "        \n",
    "    return paragraphs\n",
    "\n",
    "#End of Function\n",
    "\n",
    "para_size = 150\n",
    "num_para = 250\n",
    "\n",
    "for k, v in clean_books.items():\n",
    "    paragraphs = create_paragraphs(v, k, para_size, num_para)\n",
    "    master_paragraphs = pd.concat([master_paragraphs, paragraphs], ignore_index=True)\n",
    "    \n",
    "# To add the other column values you specified, use a dictionary and map \n",
    "    \n",
    "author_sex = {'KateChopin' : 'female', 'NathanielHawthorne': 'male', 'JackLondon': 'male', 'JohnLocke': 'male',\n",
    "              'MargaretFuller': 'female', 'JaneAustin': 'female', 'MaryWollstonecraft': 'female', \n",
    "              'VirginiaWoolf': 'female', 'MarkTwain': 'male', 'HenryDavidThoreau': 'male',  \n",
    "              'FScottFitzgerald': 'male', 'MaryShelley': 'female', 'EdithWharton': 'female', \n",
    "              'CharlesDickens': 'male'}\n",
    "\n",
    "work_period = {'KateChopin' : 'realism', 'NathanielHawthorne': 'gothic/romantic', 'JackLondon': 'naturalism', \n",
    "               'JohnLocke': 'enlightenment', 'MargaretFuller': 'transcendentalism','JaneAustin': 'victorian', \n",
    "               'MaryWollstonecraft':'enlightenment','VirginiaWoolf': 'early_modernism', \n",
    "               'MarkTwain': 'realism', 'HenryDavidThoreau': 'transcendentalism',\n",
    "               'FScottFitzgerald': 'early_modernism', 'MaryShelley': 'gothic/romantic', \n",
    "               'EdithWharton': 'naturalism', 'CharlesDickens': 'victorian'}\n",
    "\n",
    "\n",
    "master_paragraphs['sex'] = master_paragraphs['author'].map(author_sex)\n",
    "master_paragraphs['period'] = master_paragraphs['author'].map(work_period)\n",
    "\n",
    "# UNCOMMENT TO CREATE NEW DOC\n",
    "# Caution though. This will overwrite any existing csv of the same name. \n",
    "\n",
    "master_paragraphs.to_csv('{}Paragraphs_{}Words.csv'.format(num_para, para_size), mode='w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Extra-Validation Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['JosephConrad', 'FranzKafka', 'LouisaMayAlcott', 'BenjaminFranklin', 'HarrietBeecherStowe', 'HermanMelville', 'EmilyBronte', 'LewisCarroll', 'AgathaChristie', 'GertrudeStein'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_folder = Path.cwd().joinpath(\"evtextsfiles\")\n",
    "\n",
    "ev_books = {}\n",
    "\n",
    "for filename in ev_folder.iterdir():\n",
    "    with open(str(filename), encoding='utf-8', errors='ignore') as fhand:\n",
    "        text = fhand.readlines()\n",
    "        clean_text = []\n",
    "        for line in text:\n",
    "            clean_line = re.sub('[^A-Za-z0-9 ]+', '', line)\n",
    "            words = clean_line.split(\" \")\n",
    "            for word in words[:]:\n",
    "                if len(word) != 0:\n",
    "                    clean_text.append(word)\n",
    "        if len(clean_text) > 100 :\n",
    "            file_name = str(filename).split('-')[2]\n",
    "            ev_books[file_name] = ev_books.get(file_name, []) + clean_text[350:]\n",
    "        #         ev_books[file_name] = clean_text[350:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "for k, v in ev_books.items():\n",
    "    if len(v) < 1:\n",
    "        del ev_books[k]     \n",
    "\n",
    "ev_books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AgathaChristie': 3127,\n",
       " 'BenjaminFranklin': 3007,\n",
       " 'EmilyBronte': 2743,\n",
       " 'FranzKafka': 326,\n",
       " 'GertrudeStein': 4934,\n",
       " 'HarrietBeecherStowe': 6302,\n",
       " 'HermanMelville': 4042,\n",
       " 'JosephConrad': 236,\n",
       " 'LewisCarroll': 903,\n",
       " 'LouisaMayAlcott': 5197}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_word_loss_NER_stan = {}\n",
    "\n",
    "for author in ev_books.keys():\n",
    "    tagged_corpus = st.tag(ev_books[author])\n",
    "    NER_corpus =[word.lower() for word,tag in tagged_corpus if tag == 'O']\n",
    "    ev_books[author] = NER_corpus\n",
    "    ev_word_loss_NER_stan[author] = len(tagged_corpus) - len(NER_corpus)\n",
    "    \n",
    "ev_word_loss_NER_stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AgathaChristie': 4.1728385176881915,\n",
       " 'BenjaminFranklin': 3.975830336365559,\n",
       " 'EmilyBronte': 2.3743983934074304,\n",
       " 'FranzKafka': 1.5042451088962718,\n",
       " 'GertrudeStein': 5.752996595308055,\n",
       " 'HarrietBeecherStowe': 3.4972058978585028,\n",
       " 'HermanMelville': 1.9029683858666226,\n",
       " 'JosephConrad': 0.6284618662121858,\n",
       " 'LewisCarroll': 3.467875110411306,\n",
       " 'LouisaMayAlcott': 2.798720461840036}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_word_loss_NER_percents_stan = {}\n",
    "\n",
    "for author in ev_word_loss_NER_stan.keys():\n",
    "    ev_word_loss_NER_percents_stan[author] = ((ev_word_loss_NER_stan[author] / (ev_word_loss_NER_stan[author]+len(ev_books[author]))) * 100)\n",
    "\n",
    "ev_word_loss_NER_percents_stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_author_sex = {'AgathaChristie': 'female', 'JosephConrad': 'male', 'FranzKafka': 'male', \n",
    "                 'BenjaminFranklin': 'male', 'GertrudeStein': 'female', 'EmilyBronte': 'female', \n",
    "                 'HarrietBeecherStowe': 'female', 'LouisaMayAlcott': 'female', 'HermanMelville': 'male', \n",
    "                 'LewisCarroll': 'male'}\n",
    "\n",
    "ev_work_period = {'AgathaChristie' : 'early_modernism', 'JosephConrad': 'early_modernism', \n",
    "                   'FranzKafka': 'early_modernism', 'BenjaminFranklin': 'enlightenment', \n",
    "                   'GertrudeStein': 'early_modernism', 'EmilyBronte': 'victorian', \n",
    "                   'HarrietBeecherStowe':'victorian','LouisaMayAlcott': 'victorian', \n",
    "                   'HermanMelville': 'gothic/romantic', 'LewisCarroll': 'victorian'}\n",
    "\n",
    "ev_master_paragraphs = pd.DataFrame(columns = desired_columns)\n",
    "\n",
    "ev_num_para = 50\n",
    "ev_para_size = 150\n",
    "\n",
    "for k,v in ev_books.items():\n",
    "    paragraphs = create_paragraphs(v, k, ev_para_size, ev_num_para)\n",
    "    ev_master_paragraphs = pd.concat([master_paragraphs, paragraphs], ignore_index=True)\n",
    "    \n",
    "ev_master_paragraphs['sex'] = ev_master_paragraphs['author'].map(ev_author_sex)\n",
    "ev_master_paragraphs['period'] = ev_master_paragraphs['author'].map(ev_work_period)\n",
    "\n",
    "ev_master_paragraphs.to_csv('EV_{}Paragraphs_{}Words.csv'.format(ev_num_para, ev_para_size), mode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
